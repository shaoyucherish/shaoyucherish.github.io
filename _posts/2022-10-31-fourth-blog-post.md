# Fourth Blog

Varible selection is intended to select the "best" subset of predictors. The aim of variable selection is to construct a model that predicts well or explains the relationships in the data. I read three articles about variable selection in regression. There are many variable selection methods, and I conclude some methods I may use in the future.

Classical variable selection methods include forward selection, backward elimination, and stepwise selection. The names are tied with the direction of the significant variable search. Forward selection method starts with a model of size 0 and proceeds by adding variables that fulfill a defined criterion. Typically the variable to be added at each step is the one that minimizes Residual Sum of Squares (RSS) at most. Backward elimination method proceeds in the opposite way, in that it starts from a model of size p, p being the total number of variables, and eliminates not relevant variables in a step by step procedure. In this case, the variable to be deleted is usually the one that give the minimum increase in RSS. Stepwise selection is the combination of forward selection and backward elimination. 

The LASSO is a regression method which is similar to Ordinary Least Squares (OLS) regression, LASSO minimizes the Residual Sum of Squares (RSS) but poses a constraint to the sum of the absolute values of the coefficients being less than a constant. This additional constraint is moreover similar to that introduced in Ridge regression, where the constraint is to the sum of the squared values of the coefficients. This simple modification allows LASSO to perform also variable selection because the shrinkage of the coefficients is such that some coefficients can be shrunk exactly to zero.

