# Fourth Blog

Varible selection is intended to select the "best" subset of predictors. The aim of variable selection is to construct a model that predicts well or explains the relationships in the data. I read three articles about variable selection in regression. There are many variable selection methods, and I conclude some methods I may use in the future.

Classical variable selection methods include forward selection, backward elimination, and stepwise selection. The names are tied with the direction of the significant variable search. Forward selection starts with a small model, considers all one-variable expansions of the model, and adds the variable which is best according to some criterion. This criterion might be “lowest p-value”, “highest adjusted R2”, “lowest Mallow’s Cp”, “lowest AIC”, “lowest score under cross-validation”, etc. This process is then repeated, always adding one variable at a time, until the criterion stops improving. In backward elimination, we start on the contrary with the largest model we’re willing to contemplate, and keep eliminating variables until we no longer improve. Stepwise selection is the combination of forward selection and backward elimination. This addresses the situation where variables are added or removed early in the process and we want to change our mind about them later. At each stage a variable may be added or removed and there are several variations on exactly how this is done.

The LASSO is a modern variable selection method. This method has a different penalty term in its penalized estimation function. So it is also called penalty-driven method. It is similar to Ordinary Least Squares (OLS) regression, LASSO minimizes the Residual Sum of Squares (RSS) but poses a constraint to the sum of the absolute values of the coefficients being less than a constant. This additional constraint is moreover similar to that introduced in Ridge regression, where the constraint is to the sum of the squared values of the coefficients. This simple modification allows LASSO to perform also variable selection because the shrinkage of the coefficients is such that some coefficients can be shrunk exactly to zero.

References:  
[NCBI](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5969114/)  
[BioStat](https://www.biostat.jhsph.edu/~iruczins/teaching/jf/ch10.pdf)  
[Journal of Targeting](https://link.springer.com/content/pdf/10.1057/jt.2009.26.pdf)
